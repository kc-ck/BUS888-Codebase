# -*- coding: utf-8 -*-
"""LoanApproval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10h3IbUjs-7P_xVil5ZHaepghqhmTgijy

## With 3 fold CV
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import GridSearchCV

# Load the dataset
file_path = 'https://www.dropbox.com/scl/fi/hdd7i9ry4fd0bi13cbp4g/LoanApproval.csv?rlkey=6icnlcylpwd0eorkvneb5gv7k&dl=1'  # Replace with your file path
data = pd.read_csv(file_path)

# Drop Loan_ID as it's not useful for prediction
data = data.drop(columns=['Loan_ID'])

# Separate features and target
X = data.drop(columns=['Loan_Status'])
y = data['Loan_Status'].apply(lambda x: 1 if x == 'Y' else 0)  # Encode target

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['number']).columns

# Define the preprocessing for numerical data (imputing missing values)
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Define the preprocessing for categorical data (imputing and one-hot encoding)
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine the preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

# Define classifiers to be compared
classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'LASSO Logistic Regression': LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=42),
    'Ridge Logistic Regression': RidgeClassifier(alpha=1.0, random_state=42),
    'Decision Tree': DecisionTreeClassifier(criterion='entropy', max_depth = 10, random_state=42),
    'Random Forest': RandomForestClassifier(max_depth = 10, random_state=42),
    'kNN': KNeighborsClassifier(n_neighbors=7)
}

# Define the scoring metrics to be used
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'roc_auc': make_scorer(roc_auc_score)
}

# Perform 3-fold cross-validation and evaluate each classifier
results = []

# Using StratifiedKFold to maintain the same proportion of classes in each fold
kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

for name, clf in classifiers.items():
    # Create a pipeline for each classifier including preprocessing steps
    clf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', clf)])

    # Evaluate the classifier using cross-validation for each metric
    scores = cross_val_score(clf_pipeline, X, y, cv=kf, scoring='accuracy')
    accuracy = np.mean(scores)

    scores = cross_val_score(clf_pipeline, X, y, cv=kf, scoring='precision')
    precision = np.mean(scores)

    scores = cross_val_score(clf_pipeline, X, y, cv=kf, scoring='recall')
    recall = np.mean(scores)

    scores = cross_val_score(clf_pipeline, X, y, cv=kf, scoring='f1')
    f1 = np.mean(scores)

    scores = cross_val_score(clf_pipeline, X, y, cv=kf, scoring='roc_auc')
    auc = np.mean(scores)

    # Store the results for each classifier
    results.append({
        'Classifier': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'AUC': auc
    })

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame(results)

# Visualization of results
fig, ax = plt.subplots(figsize=(14, 8))

# Define the positions of the bars
x = range(len(results_df))

# Plot each metric as a separate set of bars
bar_width = 0.15
ax.bar(x, results_df['Accuracy'], width=bar_width, label='Accuracy', align='center')
ax.bar([p + bar_width for p in x], results_df['Precision'], width=bar_width, label='Precision', align='center')
ax.bar([p + 2 * bar_width for p in x], results_df['Recall'], width=bar_width, label='Recall', align='center')
ax.bar([p + 3 * bar_width for p in x], results_df['F1 Score'], width=bar_width, label='F1 Score', align='center')
ax.bar([p + 4 * bar_width for p in x], results_df['AUC'], width=bar_width, label='AUC', align='center')

# Add labels and title
ax.set_xlabel('Classifiers')
ax.set_ylabel('Scores')
ax.set_title('Comparison of Classifiers with 3-Fold Cross-Validation')
ax.set_xticks([p + 2 * bar_width for p in x])
ax.set_xticklabels(results_df['Classifier'], rotation=45, ha='right')

# Add a legend
ax.legend()

# Show the plot
plt.tight_layout()
plt.show()

print(results_df)

# SUPPOSE WE consider AUC as the best metric for comparison
best_classifier = results_df.loc[results_df['AUC'].idxmax()]    
print("Best Classifier based on AUC:")
print(best_classifier)  


# TUNING HYPERPARAMETERS FOR RIDGE CLASSIFIER

# Define the parameter grid for RidgeClassifier (lambda is alpha in sklearn)
param_grid = {
    'classifier__alpha': [0.01, 0.1, 1, 10, 100]
}

# Create a pipeline for RidgeClassifier
ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RidgeClassifier())])

# Set up GridSearchCV
grid_search = GridSearchCV(
    ridge_pipeline,
    param_grid,
    cv=kf,
    scoring='roc_auc',
    n_jobs=-1
)

# Fit GridSearchCV
grid_search.fit(X, y)

print("Best alpha (lambda) for RidgeClassifier:", grid_search.best_params_['classifier__alpha'])
print("Best cross-validated AUC:", grid_search.best_score_)